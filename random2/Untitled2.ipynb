{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn import tree\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load dataset\n",
    "dataframe = pd.DataFrame.from_csv('base_train.csv',sep=';',index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0, 42602, 42602, 42602, 42602, 42602, 42602, 42602,\n",
       "       42602, 42602, 42602, 42602, 42602, 42602, 42602, 42602, 42602,\n",
       "       42602, 42602, 42602, 42602, 42602, 42602, 42602, 42602, 42602,\n",
       "       42602, 42602, 42602, 42602, 42602, 42602, 42602, 42602, 42602,\n",
       "       42602, 42602, 42602, 42602, 42602, 42602, 42602, 42602, 42602,\n",
       "       42602, 42602, 42602, 42602, 42602, 42602, 42602, 31083, 31083])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now check that all the variables are binary\n",
    "dataset = dataframe.values\n",
    "sum((dataset == 0)*1)+sum((dataset == 1)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "report_dt        0\n",
       "ID               0\n",
       "1.1.             0\n",
       "1.2.             0\n",
       "1.3.             0\n",
       "1.4.             0\n",
       "1.5.             0\n",
       "1.6.             0\n",
       "1.7.             0\n",
       "1.8.             0\n",
       "1.9.             0\n",
       "1.10.            0\n",
       "1.11.            0\n",
       "1.12.            0\n",
       "1.13.            0\n",
       "1.14.            0\n",
       "1.15.            0\n",
       "1.16.            0\n",
       "1.17.            0\n",
       "1.18.            0\n",
       "1.19.            0\n",
       "2.1.             0\n",
       "2.2.             0\n",
       "2.3.             0\n",
       "2.4.             0\n",
       "2.5.             0\n",
       "2.6.             0\n",
       "2.7.             0\n",
       "2.8.             0\n",
       "2.9.             0\n",
       "2.10.            0\n",
       "2.11.            0\n",
       "2.12.            0\n",
       "2.13.            0\n",
       "2.14.            0\n",
       "2.15.            0\n",
       "2.16.            0\n",
       "3.1.             0\n",
       "3.2.             0\n",
       "3.3.             0\n",
       "3.4.             0\n",
       "3.5.             0\n",
       "3.6.             0\n",
       "3.7.             0\n",
       "3.8.             0\n",
       "3.9.             0\n",
       "3.10.            0\n",
       "3.11.            0\n",
       "3.12.            0\n",
       "3.13.            0\n",
       "3.14.            0\n",
       "3.15.            0\n",
       "X3           11519\n",
       "Y3           11519\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of missing inputs and outputs\n",
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing inputs, but about 25% of outputs are missing. We need to exclude those records from the dataset, as they aren't valid both for trainig and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete data with missing values\n",
    "dataframe = dataframe.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we test how the dataset is balanced, i.e. what is the proportion of 0 and 1 values of output variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "966"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of 1 values in X3 variable\n",
    "sum((dataframe['X3'] ==1)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of 1 values in Y3 variable\n",
    "sum((dataframe['Y3'] ==1)*1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that value 1 is obtained by less then 1/30 of the dataset. Which means that we'll need to apply different weights to 0 and 1 during trainig to balance classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there's no a priori information about the data we delete columns report_dt and ID. Also as we don't know the nature of the output variables (if ther's any connection between them) we use separate models to predict X3 and Y3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del dataframe['report_dt']\n",
    "del dataframe['ID']\n",
    "del dataframe['Y3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we select features that have the highest scores using chi2 function for scoring. However we should note that this is not required if we use a neural network (usually it's assumed that this step is performed by the network itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate scores of featuress\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "values = SelectKBest(chi2, k='all').fit(dataframe.drop('X3',axis=1).values,dataframe['X3'].values)\n",
    "scores = -numpy.log10(values.pvalues_)\n",
    "\n",
    "# plot results\n",
    "#%matplotlib inline\n",
    "#plt.bar(range(len(dataframe.columns)-1), scores)\n",
    "#plt.xticks(range(len(dataframe.columns)-1), dataframe.columns, rotation='vertical')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we leave 20 features with highest scores and drop others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_reduced = SelectKBest(chi2, k=20).fit_transform(dataframe.drop('X3',axis=1).values,dataframe['X3'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_reduced = numpy.append(dataset_reduced,numpy.reshape(dataframe['X3'].values,(len(dataframe['X3'].values),1)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step we split data into training and test sets in proportion 3/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into trainig (75%) and test (25%) sets\n",
    "train, test = sk.cross_validation.train_test_split(dataset_reduced, test_size = 0.25)\n",
    "# split into input (X) and (Y) variables\n",
    "Xtrain = train[:,0:dataset_reduced.shape[1]-1]\n",
    "Ytrain = train[:,dataset_reduced.shape[1]-1]\n",
    "Xtest = test[:,0:dataset_reduced.shape[1]-1]\n",
    "Ytest = test[:,dataset_reduced.shape[1]-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build the decision tree model using sklearn framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            random_state=None, splitter='best')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit model\n",
    "modeltree = tree.DecisionTreeClassifier()\n",
    "modeltree.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prediction on test set\n",
    "Ypredtree = modeltree.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next model is the SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit model\n",
    "modelsvm = svm.SVC()\n",
    "modelsvm.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make prediction on test set\n",
    "Ypredsvm = modelsvm.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function that evaluates performance of the model through different metrics. Short description for each metric is given in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluation(Xtest,Ytest,Ypred):\n",
    "    \n",
    "    # accuracy on test set\n",
    "    score = sk.metrics.accuracy_score(Ytest,Ypred)\n",
    "    print 'accuracy:', round(score*100,2), '%'\n",
    "    \n",
    "    # precision on test set\n",
    "    # The precision is the ratio tp / (tp + fp) where tp is the\n",
    "    # number of true positives and fp the number of false positives.\n",
    "    # The best value is 1 and the worst value is 0.\n",
    "    score = sk.metrics.precision_score(Ytest,Ypred)\n",
    "    print 'precision:', round(score*100,2), '%' \n",
    "    \n",
    "    # recall on test set\n",
    "    # The recall is the ratio tp / (tp + fn) where tp is the number of\n",
    "    # true positives and fn the number of false negatives.\n",
    "    # The best value is 1 and the worst value is 0.\n",
    "    score = sk.metrics.recall_score(Ytest,Ypred)\n",
    "    print 'recall:', round(score*100,2), '%'\n",
    "    \n",
    "    # f1 score on test set\n",
    "    # The F1 score can be interpreted as a weighted average of the \n",
    "    # precision and recall, where an F1 score reaches its best value at\n",
    "    # 1 and worst score at 0.\n",
    "    score = sk.metrics.f1_score(Ytest,Ypred)\n",
    "    print 'f1:', score\n",
    "    \n",
    "    # Area Under the Curve from prediction scores\n",
    "    # The best performance is 1\n",
    "    score = sk.metrics.roc_auc_score(Ytest,Ypred)\n",
    "    print 'AUC:', score  \n",
    "    \n",
    "    # Confusion matrix\n",
    "    # By definition a confusion matrix C is such that C_{i, j} \n",
    "    # is equal to the number of observations known to be in group i \n",
    "    # but predicted to be in group j.\n",
    "    print 'Cofusion matrix:'\n",
    "    print sk.metrics.confusion_matrix(Ytest,Ypred)\n",
    "    \n",
    "    # Matthews correlation coefficient (phi coefficient)\n",
    "    # Is used in machine learning \n",
    "    # as a measure of the quality of binary (two-class) classifications.\n",
    "    # It takes into account true and false positives and negatives and \n",
    "    # is generally regarded as a balanced measure which can be used even\n",
    "    # if the classes are of very different sizes. \n",
    "    #+1 perfect prediction; 0 random prediction; -1 inverse prediction\n",
    "    score = sk.metrics.matthews_corrcoef(Ytest,Ypred)\n",
    "    print 'MCC:', score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 98.24 %\n",
      "precision: 76.27 %\n",
      "recall: 58.7 %\n",
      "f1: 0.663390663391\n",
      "AUC: 0.790693484315\n",
      "Cofusion matrix:\n",
      "[[7499   42]\n",
      " [  95  135]]\n",
      "MCC: 0.660422334399\n"
     ]
    }
   ],
   "source": [
    "evaluation(Xtest,Ytest,Ypredtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the svm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 97.85 %\n",
      "precision: 66.32 %\n",
      "recall: 55.65 %\n",
      "f1: 0.605200945626\n",
      "AUC: 0.773951096326\n",
      "Cofusion matrix:\n",
      "[[7476   65]\n",
      " [ 102  128]]\n",
      "MCC: 0.596658005283\n"
     ]
    }
   ],
   "source": [
    "evaluation(Xtest,Ytest,Ypredsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
